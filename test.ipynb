{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: UTF-8\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from train_eval import train\n",
    "import argparse\n",
    "from config import Config\n",
    "from utils import build_dataset, build_iterator\n",
    "import PEAN_model\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Traffic Classification')\n",
    "parser.add_argument('--pad_num', type=int, default=10, help='the padding size of packet num')\n",
    "parser.add_argument('--pad_len', default=400, type=int, help='the padding size(length) of each packet')\n",
    "parser.add_argument('--pad_len_seq', default=10, type=int, help='the padding size of packet length sequence')\n",
    "parser.add_argument('--emb', default=128, type=int, help='the emb size of bytes')\n",
    "parser.add_argument('--device', default='cuda:0', type=str, help='the training device')\n",
    "parser.add_argument('--load', default=False, type=bool, help='whether train on previous model')\n",
    "parser.add_argument('--batch', default=64, type=int, help='batch_size')\n",
    "parser.add_argument('--feature', default='ensemble', type=str, help='length / raw / ensemble')\n",
    "parser.add_argument('--method', default='trf', type=str, help='lstm / trf (Sequential Layer)')\n",
    "parser.add_argument('--embway', default='random', type=str, help='random / pretrain (for raw)')\n",
    "parser.add_argument('--imploss', default=False, type=bool, help='whether to use improved loss')\n",
    "parser.add_argument('--lr', default=1e-3, type=float, help='learning rate')\n",
    "parser.add_argument('--length_emb_size', default=32, type=int, help='len emb size')\n",
    "parser.add_argument('--lenhidden', default=128, type=int, help='len hidden size')\n",
    "parser.add_argument('--embhidden', default=1024, type=int, help='emb hidden size')\n",
    "parser.add_argument('--seed', default=1, type=int, help='random seed')\n",
    "parser.add_argument('--trf_heads', default=8, type=int, help='transformers heads number')\n",
    "parser.add_argument('--trf_layers', default=2, type=int, help='transformers layers')\n",
    "parser.add_argument('--mode', default='train', type=str, help='train/test')\n",
    "parser.add_argument('--k', default='10', type=int, help='k fold validation')\n",
    "parser.add_argument('--epoch', default='300', type=int, help='epoch')\n",
    "args = parser.parse_args()\n",
    "\n",
    "def get_k_fold_data(k, i, X):\n",
    "    assert k > 1\n",
    "    fold_size = len(X) // k\n",
    "\n",
    "    X_train = None\n",
    "    for j in range(k):\n",
    "        X_part = X[j * fold_size: (j + 1) * fold_size]\n",
    "        if j == i:\n",
    "            X_valid = X_part\n",
    "        elif X_train is None:\n",
    "            X_train = X_part\n",
    "        else:\n",
    "            X_train = X_train + X_part\n",
    "    return X_train, X_valid\n",
    "\n",
    "def get_model(config):\n",
    "    return PEAN_model.PEAN(config).to(config.device)\n",
    "\n",
    "def get_config():\n",
    "    config = Config()\n",
    "    config.pad_num = args.pad_num\n",
    "    config.pad_length = args.pad_len\n",
    "    config.pad_len_seq = args.pad_len_seq\n",
    "    config.device = torch.device(args.device if torch.cuda.is_available() else 'cpu')  # 设备\n",
    "    config.mode = args.mode\n",
    "    config.embedding_size = args.emb\n",
    "    config.batch_size = args.batch\n",
    "    config.load = args.load\n",
    "    config.lenlstmhidden_size = args.lenhidden\n",
    "    config.emblstmhidden_size = args.embhidden\n",
    "    config.feature = args.feature\n",
    "    config.method = args.method\n",
    "    config.embway = args.embway\n",
    "    config.length_emb_size = args.length_emb_size\n",
    "    config.imploss = args.imploss\n",
    "    config.learning_rate = args.lr\n",
    "    config.seed = args.seed\n",
    "    config.trf_heads = args.trf_heads\n",
    "    config.trf_layers = args.trf_layers\n",
    "    config.k = args.k\n",
    "    config.num_epochs = args.epoch\n",
    "    if args.mode == \"test\":\n",
    "        config.load = True\n",
    "        config.num_epochs = 0\n",
    "\n",
    "    name = \"{}_{}\".format(config.feature, config.seed)\n",
    "    if config.feature != \"length\":\n",
    "        name += \"_{}_{}_{}_{}_{}_{}\".format(config.embway, config.method, config.embedding_size, config.pad_num,\n",
    "                                            config.pad_length, config.emblstmhidden_size)\n",
    "    if config.feature == \"length\" or config.feature == \"ensemble\":\n",
    "        name += \"_{}_{}\".format(config.pad_len_seq, config.lenlstmhidden_size)\n",
    "    if config.method == \"trf\":\n",
    "        if config.trf_heads == 8 and config.trf_layers == 2:\n",
    "            pass\n",
    "        else:\n",
    "            name += \"_{}_{}\".format(config.trf_heads, config.trf_layers)\n",
    "    if config.imploss:\n",
    "        name += \"_imploss\"\n",
    "    config.print_path = config.record_path + name + \".txt\"  # record console log\n",
    "    config.loss_path = config.loss_path + name + \".txt\"     # record loss\n",
    "    config.save_path = config.save_path + name + \".ckpt\"    # record saved model\n",
    "    print(\"\\nModel save at: \", config.save_path)\n",
    "    from transformers import BertTokenizer\n",
    "    config.tokenizer = BertTokenizer(vocab_file=config.vocab_path, max_seq_length=config.pad_num - 2, max_len=config.pad_num)\n",
    "\n",
    "    return config\n",
    "\n",
    "def prepare_data():\n",
    "    print(\"----------------------------\\n\")\n",
    "\n",
    "    with open(config.print_path, 'a') as f:\n",
    "        f.write(\"----------------------------\\n\\n\")\n",
    "\n",
    "    msg = \"Iput Feature: {}\\nRandom Seed: {}\\n\".format(config.feature, config.seed)\n",
    "    if args.feature == \"raw\" or args.feature == \"ensemble\":\n",
    "        msg += \"Sequential use: {}\\n\".format(config.method)\n",
    "        msg += \"Embedding way: {}(hidden:{})\\n\".format(config.embway, config.emblstmhidden_size)\n",
    "        if config.method == \"pretrain\":\n",
    "            msg += \"Bert Size: {}\\n\".format(config.bert_dim)\n",
    "        else:\n",
    "            msg += \"Embedding Size: {}\\n\".format(config.embedding_size)\n",
    "        msg += \"Pad_num: {}\\n\".format(config.pad_num)\n",
    "        msg += \"Pad_len: {}\\n\".format(config.pad_length)\n",
    "\n",
    "    if config.feature == \"length\" or args.feature == \"ensemble\":\n",
    "        msg += \"Length use: lstm(emb: {}, hidden:{})\\n\".format(config.length_emb_size, config.lenlstmhidden_size)\n",
    "        msg += \"Pad_len_seq: {}\\n\".format(config.pad_len_seq)\n",
    "\n",
    "    if config.method == \"trf\":\n",
    "        msg += \"trf heads:{}\\n\".format(config.trf_heads)\n",
    "        msg += \"trf_layers: {}\\n\".format(config.trf_layers)\n",
    "\n",
    "    msg += \"Use Improved loss: {}\\n\".format(config.imploss)\n",
    "    msg += \"Learning Rate: {}\\n\".format(config.learning_rate)\n",
    "    msg += \"Batch Size:{}\\n\".format(config.batch_size)\n",
    "\n",
    "    print(msg)\n",
    "    with open(config.print_path, 'a') as f:\n",
    "        f.write(msg)\n",
    "    print(\"----------------------------\\n\")\n",
    "    with open(config.print_path, 'a') as f:\n",
    "        f.write(\"----------------------------\\n\\n\")\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "    with open(config.print_path, 'a') as f:\n",
    "        f.write(\"Loading data...\\n\")\n",
    "\n",
    "\n",
    "    train_data = build_dataset(config)\n",
    "\n",
    "    print(\"train_set: {}\".format(len(train_data)))\n",
    "    with open(config.print_path, 'a') as f:\n",
    "        f.write(\"train_set: {}\\n\".format(len(train_data)))\n",
    "    return train_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = prepare_data()\n",
    "\n",
    "valid_loss, valid_acc, valid_fpr, valid_tpr, valid_ftf, valid_f1 = 0, 0, 0, 0, 0, 0\n",
    "model = get_model(config)\n",
    "print(model.parameters, \"\\n\")\n",
    "print(\"Total number of paramerters in networks is {}  \".format(sum(x.numel() for x in model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(config.k):\n",
    "    train_, test_ = get_k_fold_data(config.k, i, train_data)\n",
    "    model = get_model(config)\n",
    "\n",
    "    train_iter = build_iterator(train_, config)\n",
    "    test_iter = build_iterator(test_, config)\n",
    "    dev_iter = build_iterator(test_, config)\n",
    "    acc_, loss_, f1_, fpr_, tpr_, ftf_ = train(config, model, train_iter, dev_iter, test_iter)\n",
    "\n",
    "    print('*' * 25, 'result of', i + 1, 'fold', '*' * 25)\n",
    "    print('loss:%.6f' % loss_, 'acc:%.4f' % acc_, 'FPR:%.4f' % fpr_, 'TPR:%.4f' % tpr_, 'FTF:%.4f' % ftf_, 'F1-macro:%.4f' % f1_, \"\\n\")\n",
    "    valid_loss += loss_\n",
    "    valid_acc += acc_\n",
    "    valid_fpr += fpr_\n",
    "    valid_tpr += tpr_\n",
    "    valid_ftf += ftf_\n",
    "    valid_f1 += f1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\", '#' * 10, 'final result of all k fold', '#' * 10)\n",
    "print('acc:%.4f' % (valid_acc/config.k), 'F1-macro:%.4f' % (valid_f1/config.k), \\\n",
    "        'TPR:%.4f' % (valid_tpr/config.k), 'FPR:%.4f' % (valid_fpr/config.k), \\\n",
    "        'FTF:%.4f' % (valid_ftf/config.k), 'loss:%.6f' % (valid_loss/config.k), \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
